{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf3f43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows kept: 11492 | Dropped (invalid labels): 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Code\\MetaLanguageBias\\language-bias\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\barif\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Tokenizing: 100%|██████████| 11492/11492 [00:00<00:00, 15835.49 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 11492/11492 [00:00<00:00, 964501.08 examples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tokenized dataset to: c:\\Code\\MetaLanguageBias\\language-bias\\data\\tokenized_bert_uncased_max128\n",
      "Example tokens: ['women', ',', 'doctors', ',', 'and', 'nurses', '!', '?']\n",
      "Loaded OK. Columns: ['input_ids', 'token_type_ids', 'attention_mask', 'labels', 'category']\n",
      "First row (truncated): {'input_ids': [101, 10047, 3374, 2023, 2003, 6230, 2005, 2017, 1012, 2012], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0, 'category': 'gender_female'}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, pandas as pd\n",
    "from datasets import Dataset, load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_IN = PROJECT_ROOT / \"data\" / \"cleaned_data.csv\"\n",
    "DATA_OUT_DIR = PROJECT_ROOT / \"data\" / \"tokenized_bert_uncased_max128\"\n",
    "MAX_LEN = 128\n",
    "REMOVE_STOPWORDS = False\n",
    "\n",
    "assert DATA_IN.exists(), f\"Missing file: {DATA_IN}\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(DATA_IN, usecols=[\"comment\", \"bias_sent\", \"category\"]).dropna(subset=[\"comment\",\"bias_sent\"])\n",
    "\n",
    "labels_num = pd.to_numeric(df[\"bias_sent\"], errors=\"coerce\")\n",
    "mask = labels_num.isin([0, 1])\n",
    "dropped = (~mask | labels_num.isna()).sum()\n",
    "df = df.loc[mask, [\"comment\", \"category\"]].copy()\n",
    "df[\"labels\"] = labels_num.loc[mask].astype(int)\n",
    "\n",
    "\n",
    "def maybe_remove_stopwords(text: str) -> str:\n",
    "    if not REMOVE_STOPWORDS:\n",
    "        return str(text)\n",
    "    toks = [w for w in str(text).split() if w.lower() not in ENGLISH_STOP_WORDS]\n",
    "    return \" \".join(toks)\n",
    "\n",
    "df[\"text\"] = df[\"comment\"].astype(str).apply(maybe_remove_stopwords)\n",
    "\n",
    "print(f\"Rows kept: {len(df)} | Dropped (invalid labels): {int(dropped)}\")\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
    "\n",
    "\n",
    "ds = Dataset.from_pandas(df[[\"text\", \"labels\", \"category\"]], preserve_index=False)\n",
    "\n",
    "def tok(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "ds = ds.map(tok, batched=True, desc=\"Tokenizing\")\n",
    "\n",
    "# keep standard columns\n",
    "keep = [\"input_ids\", \"attention_mask\", \"labels\", \"category\"]\n",
    "if \"token_type_ids\" in ds.column_names:\n",
    "    keep.insert(1, \"token_type_ids\")\n",
    "ds = ds.select_columns(keep)\n",
    "\n",
    "\n",
    "DATA_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ds.save_to_disk(str(DATA_OUT_DIR))\n",
    "print(f\"Saved tokenized dataset to: {DATA_OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caea70e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels', 'category'],\n",
      "    num_rows: 11492\n",
      "})\n",
      "Row 0 label/category: {'labels': 0, 'category': 'gender_female'}\n",
      "Max seq len: 128\n"
     ]
    }
   ],
   "source": [
    "loaded = load_from_disk(str(DATA_OUT_DIR))\n",
    "print(loaded)  # or loaded[\"train\"] if you saved a DatasetDict\n",
    "row0 = {k: loaded[k][0] for k in [\"labels\",\"category\"]}\n",
    "print(\"Row 0 label/category:\", row0)\n",
    "max_len = max(len(ids) for ids in loaded[\"input_ids\"]) if \"input_ids\" in loaded.column_names else 0\n",
    "print(\"Max seq len:\", max_len)  # should be <= 128\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
