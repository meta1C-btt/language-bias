{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf3f43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os, pandas as pd\n",
    "from datasets import Dataset, load_from_disk\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "DATA_IN = PROJECT_ROOT / \"data\" / \"cleaned_data.csv\"\n",
    "DATA_OUT_DIR = PROJECT_ROOT / \"data\" / \"tokenized_bert_uncased_max128\"\n",
    "MAX_LEN = 128\n",
    "REMOVE_STOPWORDS = False\n",
    "\n",
    "assert DATA_IN.exists(), f\"Missing file: {DATA_IN}\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(DATA_IN, usecols=[\"comment\", \"bias_sent\", \"category\"]).dropna(subset=[\"comment\",\"bias_sent\"])\n",
    "\n",
    "labels_num = pd.to_numeric(df[\"bias_sent\"], errors=\"coerce\")\n",
    "mask = labels_num.isin([0, 1])\n",
    "dropped = (~mask | labels_num.isna()).sum()\n",
    "df = df.loc[mask, [\"comment\", \"category\"]].copy()\n",
    "df[\"labels\"] = labels_num.loc[mask].astype(int)\n",
    "\n",
    "\n",
    "def maybe_remove_stopwords(text: str) -> str:\n",
    "    if not REMOVE_STOPWORDS:\n",
    "        return str(text)\n",
    "    toks = [w for w in str(text).split() if w.lower() not in ENGLISH_STOP_WORDS]\n",
    "    return \" \".join(toks)\n",
    "\n",
    "df[\"text\"] = df[\"comment\"].astype(str).apply(maybe_remove_stopwords)\n",
    "\n",
    "print(f\"Rows kept: {len(df)} | Dropped (invalid labels): {int(dropped)}\")\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
    "\n",
    "\n",
    "ds = Dataset.from_pandas(df[[\"text\", \"labels\", \"category\"]], preserve_index=False)\n",
    "\n",
    "def tok(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "ds = ds.map(tok, batched=True, desc=\"Tokenizing\")\n",
    "\n",
    "# keep standard columns\n",
    "keep = [\"input_ids\", \"attention_mask\", \"labels\", \"category\"]\n",
    "if \"token_type_ids\" in ds.column_names:\n",
    "    keep.insert(1, \"token_type_ids\")\n",
    "ds = ds.select_columns(keep)\n",
    "\n",
    "\n",
    "DATA_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ds.save_to_disk(str(DATA_OUT_DIR))\n",
    "print(f\"Saved tokenized dataset to: {DATA_OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caea70e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reloading the saved dataset from disk to confirm its readable\n",
    "#Prints a summary object that shows columns and number of rows\n",
    "\n",
    "loaded = load_from_disk(str(DATA_OUT_DIR))\n",
    "print(loaded) \n",
    "row0 = {k: loaded[k][0] for k in [\"labels\",\"category\"]}\n",
    "print(\"Row 0 label/category:\", row0)\n",
    "max_len = max(len(ids) for ids in loaded[\"input_ids\"]) if \"input_ids\" in loaded.column_names else 0\n",
    "print(\"Max seq len:\", max_len) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
